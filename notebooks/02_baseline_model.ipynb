{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "cells": [
  {
   "id": "a45deb41",
   "cell_type": "markdown",
   "source": "# 02_baseline_model.ipynb\n\n## MAP - Charting Student Math Misunderstandings  \n**Baseline modeling (classical ML)**\n\nThis notebook builds a strong, fast baseline using:\n- text concatenation: `QuestionText + MC_Answer + StudentExplanation`\n- TF‑IDF features\n- Linear classifier (Logistic Regression / Linear SVM)\n- validation split (stratified)\n- submission file generation: `Category:Misconception`\n",
   "metadata": {}
  },
  {
   "id": "48d83698",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "import sys\nfrom pathlib import Path\n\n# Project root (assuming notebooks/ is alongside src/ and data/)\nROOT_DIR = Path().resolve().parent\nsys.path.append(str(ROOT_DIR))\n\nprint(\"ROOT_DIR:\", ROOT_DIR)\n",
   "outputs": []
  },
  {
   "id": "f2914037",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "import numpy as np\nimport pandas as pd\n\nfrom src.data_load import load_train, load_test\n\ntrain = load_train()\ntest = load_test()\n\nprint(\"train shape:\", train.shape)\nprint(\"test shape :\", test.shape)\n\ntrain.head()\n",
   "outputs": []
  },
  {
   "id": "9d248bea",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Column normalization (robust to minor naming differences) ---\ndef normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    rename_map = {}\n\n    # Common variants we might encounter\n    candidates = {\n        \"StudentExplanation\": [\"Student Explanation\", \"Student_Explanation\", \"StudentExplanation\", \"student_explanation\"],\n        \"QuestionText\": [\"QuestionText\", \"Question Text\", \"question_text\"],\n        \"MC_Answer\": [\"MC_Answer\", \"MC Answer\", \"MCAnswer\", \"mc_answer\"],\n        \"QuestionId\": [\"QuestionId\", \"QuestionID\", \"question_id\"],\n        \"Category\": [\"Category\", \"category\"],\n        \"Misconception\": [\"Misconception\", \"misconception\"],\n    }\n\n    for target, variants in candidates.items():\n        for v in variants:\n            if v in df.columns:\n                rename_map[v] = target\n                break\n\n    df = df.rename(columns=rename_map)\n    return df\n\ntrain = normalize_columns(train)\ntest = normalize_columns(test)\n\nrequired_cols = [\"QuestionId\", \"QuestionText\", \"MC_Answer\", \"StudentExplanation\"]\nmissing_train = [c for c in required_cols if c not in train.columns]\nmissing_test  = [c for c in required_cols if c not in test.columns]\n\nprint(\"Missing in train:\", missing_train)\nprint(\"Missing in test :\", missing_test)\n\ntrain.columns.tolist()\n",
   "outputs": []
  },
  {
   "id": "1bea9b87",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Target construction: Category:Misconception ---\n# Misconception is only meaningful when Category indicates misconception; otherwise it is NA.\ndef make_catmis(df: pd.DataFrame) -> pd.Series:\n    cat = df[\"Category\"].astype(str)\n    if \"Misconception\" in df.columns:\n        mc = df[\"Misconception\"]\n        mc = mc.where(mc.notna(), \"NA\").astype(str)\n    else:\n        mc = pd.Series([\"NA\"] * len(df), index=df.index)\n\n    return cat + \":\" + mc\n\nif \"Category\" not in train.columns:\n    raise ValueError(\"Train data must contain 'Category' for supervised baseline.\")\n\ntrain[\"CatMis\"] = make_catmis(train)\ntrain[\"CatMis\"].value_counts().head(10)\n",
   "outputs": []
  },
  {
   "id": "1712686a",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Text building ---\ndef build_text(df: pd.DataFrame) -> pd.Series:\n    q = df[\"QuestionText\"].fillna(\"\").astype(str)\n    a = df[\"MC_Answer\"].fillna(\"\").astype(str)\n    e = df[\"StudentExplanation\"].fillna(\"\").astype(str)\n    # Using explicit tags often helps linear models\n    return (\"[Q] \" + q + \" [A] \" + a + \" [E] \" + e)\n\ntrain[\"text\"] = build_text(train)\ntest[\"text\"]  = build_text(test)\n\ntrain[\"text\"].str.len().describe()\n",
   "outputs": []
  },
  {
   "id": "073066ae",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Train/Validation split (stratified) ---\nfrom sklearn.model_selection import train_test_split\n\nX = train[\"text\"].values\ny = train[\"CatMis\"].values\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\nprint(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n",
   "outputs": []
  },
  {
   "id": "5bd8af6e",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Baseline 0: Majority class ---\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score, f1_score\n\nmajor = Counter(y_train).most_common(1)[0][0]\ny_pred_major = np.array([major] * len(y_val))\n\nprint(\"Majority label:\", major)\nprint(\"Accuracy:\", accuracy_score(y_val, y_pred_major))\nprint(\"Macro F1:\", f1_score(y_val, y_pred_major, average=\"macro\"))\n",
   "outputs": []
  },
  {
   "id": "1e1e18fa",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Baseline 1: TF‑IDF + Logistic Regression ---\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\ntfidf_lr = Pipeline([\n    (\"tfidf\", TfidfVectorizer(\n        ngram_range=(1,2),\n        min_df=2,\n        max_df=0.95,\n        sublinear_tf=True\n    )),\n    (\"clf\", LogisticRegression(\n        max_iter=2000,\n        n_jobs=-1,\n        class_weight=\"balanced\"  # helps with label imbalance\n    ))\n])\n\ntfidf_lr.fit(X_train, y_train)\npred_lr = tfidf_lr.predict(X_val)\n\nprint(\"Accuracy:\", accuracy_score(y_val, pred_lr))\nprint(\"Macro F1:\", f1_score(y_val, pred_lr, average=\"macro\"))\nprint()\nprint(classification_report(y_val, pred_lr, digits=4))\n",
   "outputs": []
  },
  {
   "id": "b4ca4abe",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Optional: TF‑IDF + Linear SVM (often strong for text) ---\nfrom sklearn.svm import LinearSVC\n\ntfidf_svm = Pipeline([\n    (\"tfidf\", TfidfVectorizer(\n        ngram_range=(1,2),\n        min_df=2,\n        max_df=0.95,\n        sublinear_tf=True\n    )),\n    (\"clf\", LinearSVC(class_weight=\"balanced\"))\n])\n\ntfidf_svm.fit(X_train, y_train)\npred_svm = tfidf_svm.predict(X_val)\n\nprint(\"Accuracy:\", accuracy_score(y_val, pred_svm))\nprint(\"Macro F1:\", f1_score(y_val, pred_svm, average=\"macro\"))\n",
   "outputs": []
  },
  {
   "id": "e1026185",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Pick the best model (by macro F1) for submission\nbest_model = tfidf_lr\nbest_name = \"tfidf_lr\"\n\nf1_lr = f1_score(y_val, pred_lr, average=\"macro\")\nf1_svm = f1_score(y_val, pred_svm, average=\"macro\")\n\nif f1_svm > f1_lr:\n    best_model = tfidf_svm\n    best_name = \"tfidf_svm\"\n\nprint(\"Best model:\", best_name, \"| macro F1:\", max(f1_lr, f1_svm))\n",
   "outputs": []
  },
  {
   "id": "7e9d6cc5",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Train on full training data ---\nbest_model.fit(train[\"text\"].values, train[\"CatMis\"].values)\n",
   "outputs": []
  },
  {
   "id": "e8e63d52",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- Predict on test and create submission ---\ntest_pred = best_model.predict(test[\"text\"].values)\n\nsub = pd.DataFrame({\n    \"Category:Misconception\": test_pred\n})\n\n# If your competition expects an id column, uncomment and adjust:\n# sub.insert(0, \"QuestionId\", test[\"QuestionId\"].values)\n\nout_path = ROOT_DIR / \"submission.csv\"\nsub.to_csv(out_path, index=False)\n\nout_path, sub.head()\n",
   "outputs": []
  },
  {
   "id": "bc9e6241",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# --- (Optional) Save model artifact ---\nimport joblib\n\nmodel_path = ROOT_DIR / f\"{best_name}.joblib\"\njoblib.dump(best_model, model_path)\n\nmodel_path\n",
   "outputs": []
  },
  {
   "id": "ce581254",
   "cell_type": "markdown",
   "source": "## Notes / Next improvements\n- Try different text templates (e.g., `QuestionText + StudentExplanation` only).\n- Add char‑level ngrams or use `HashingVectorizer` for speed.\n- Move to a Transformer baseline (e.g., DeBERTa/RoBERTa) once classical baseline is established.\n- Consider a **two‑stage baseline**: predict Category first, then Misconception only when Category indicates misconception.\n",
   "metadata": {}
  }
 ]
}