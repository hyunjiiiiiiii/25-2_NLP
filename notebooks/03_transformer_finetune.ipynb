{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c86ae8-64f0-42bc-b7fe-c6f73cd0ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from src.data_load import load_train, load_test\n",
    "from src.preprocessing import add_text_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdcde70-81b5-4aed-b81f-dedbdc625d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a94c2e-d6a9-404d-8aa0-77c41f481387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73cfa99-4727-4de7-8ced-3b8c0bcf66ab",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635f0d44-b553-49c7-91e3-9210ea29b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_train()\n",
    "test = load_test()\n",
    "\n",
    "# text column 생성\n",
    "train = add_text_column(train)\n",
    "test = add_text_column(test)\n",
    "\n",
    "# target 생성\n",
    "train[\"Misconception\"] = train[\"Misconception\"].fillna(\"NA\").astype(str)\n",
    "train[\"CatMis\"] = train[\"Category\"].astype(str) + \":\" + train[\"Misconception\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8945fc1-5989-4cba-a28b-57c3ac0cbf58",
   "metadata": {},
   "source": [
    "### 2. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14ee020-6c21-423d-91a7-d2599472730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train[\"label\"] = le.fit_transform(train[\"CatMis\"])\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "print(\"Number of labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdea587-1df5-483a-80f6-3186ff9184d2",
   "metadata": {},
   "source": [
    "### 3. Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc87778-1944-4798-8723-a2c78b3da1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 29356 Val: 7340\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Val:\", len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310bb86-946c-458e-ac83-c74b36647166",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a59a9a9-16ed-4840-9c1a-38d64431daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/deberta-v3-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6fd0cc4-7141-40e9-a6a3-f58fcdba8b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\hj\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ebea1f7-7aa7-4fe6-93ed-12095a3cd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f40fedd-08cb-4a4b-a80f-bf23f6387737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e890f4a1c2e4a8b82cc3fa9ab3fa442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f7a69165bc4b9092c757a2d3a32b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset 정의\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_pandas(\n",
    "    train_df[[\"text\", \"label\"]]\n",
    ").map(tokenize, batched=True)\n",
    "\n",
    "val_ds = Dataset.from_pandas(\n",
    "    val_df[[\"text\", \"label\"]]\n",
    ").map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c856f75-6ef2-4c41-bbd0-f875c5b90ddd",
   "metadata": {},
   "source": [
    "### 5. Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85def9db-37e7-4c3e-9406-8140846a19f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77425131-3d51-43b3-9048-42db418324be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "743666eb-542f-455b-976d-e21890545b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c377dd5-0be3-4600-a500-3b6bc520bdd0",
   "metadata": {},
   "source": [
    "### 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b5d12e-fb52-45fd-9c3d-7d909d730980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0daee967-df45-471d-8853-fe06dfb3f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a55e8306-0da3-41a4-904b-b62bd2caa289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8551b7b0-9d92-46f9-9b7a-4b7a3d4b10d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2044\\2883113001.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3838bf15-041f-483c-9e41-edc88fa7afff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5505' max='5505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5505/5505 09:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.831900</td>\n",
       "      <td>0.765560</td>\n",
       "      <td>0.693052</td>\n",
       "      <td>0.219501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.753500</td>\n",
       "      <td>0.736559</td>\n",
       "      <td>0.696730</td>\n",
       "      <td>0.279355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.753100</td>\n",
       "      <td>0.714758</td>\n",
       "      <td>0.704360</td>\n",
       "      <td>0.287614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5505, training_loss=0.8938200467288981, metrics={'train_runtime': 554.3319, 'train_samples_per_second': 158.872, 'train_steps_per_second': 9.931, 'total_flos': 3657693501481272.0, 'train_loss': 0.8938200467288981, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce509285-a5a3-4773-a1b0-b597dd5f2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class-weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c667e7bf-f6ea-4e91-bde7-b481065ad378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "num_labels = train[\"CatMis\"].nunique()\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7deacf90-4879-4302-ab7c-a6175e42159f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0047, 0.0040, 0.0072, 0.0024, 0.0165, 0.0165, 0.0094, 0.0039, 0.0093,\n",
       "         0.0028]),\n",
       " tensor(1.),\n",
       " tensor(63.8073))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# y_train은 정수 라벨(0~64)이어야 함\n",
    "# 만약 train_df[\"label\"]이 있다면 그걸 쓰는게 가장 안전\n",
    "y_train_labels = np.array(train_df[\"label\"].values)\n",
    "\n",
    "num_labels = 65\n",
    "counts = np.bincount(y_train_labels, minlength=num_labels)\n",
    "\n",
    "# inverse frequency (너무 극단적이면 학습 불안정 → sqrt나 log를 추천)\n",
    "weights = 1.0 / np.sqrt(counts + 1e-6)\n",
    "\n",
    "# normalize (평균 1로 맞추면 안정적)\n",
    "weights = weights / weights.mean()\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "class_weights[:10], class_weights.mean(), class_weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f7e7d75-485b-4bd7-a969-85b5517a6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.clamp(class_weights, max=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20e2fd0b-ba91-4643-a406-74dac9aacb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            cw = self.class_weights.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=cw)\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58258579-52ab-4962-b5a5-0e3be45b91cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['text',\n",
       "  'labels',\n",
       "  '__index_level_0__',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask'],\n",
       " ['text',\n",
       "  'labels',\n",
       "  '__index_level_0__',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = train_ds.rename_column(\"label\", \"labels\") if \"label\" in train_ds.column_names else train_ds\n",
    "val_ds   = val_ds.rename_column(\"label\", \"labels\") if \"label\" in val_ds.column_names else val_ds\n",
    "\n",
    "train_ds.column_names, val_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d017a8f-aa72-4002-8a78-9dcfce02c27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2044\\1965059423.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "weighted_trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b0e50c7-6556-4229-bec8-2b69a10024fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5505' max='5505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5505/5505 09:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>1.032244</td>\n",
       "      <td>0.666213</td>\n",
       "      <td>0.342737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.043200</td>\n",
       "      <td>0.966088</td>\n",
       "      <td>0.661444</td>\n",
       "      <td>0.359357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.001400</td>\n",
       "      <td>0.965110</td>\n",
       "      <td>0.659128</td>\n",
       "      <td>0.367652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5505, training_loss=0.9784520681937319, metrics={'train_runtime': 556.2018, 'train_samples_per_second': 158.338, 'train_steps_per_second': 9.897, 'total_flos': 3657693501481272.0, 'train_loss': 0.9784520681937319, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b347bf-47e9-46ac-84f7-d497d27e2ebc",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "- unweighted\n",
    "    - Accuracy ≈ 0.70\n",
    "    - Macro F1 ≈ 0.29\n",
    "    - Loss ≈ 0.71\n",
    "\n",
    "- class-weighted\n",
    "    - Accuracy ≈ 0.66\n",
    "    - Macro F1 ≈ 0.37 (+0.08 이상 상승)\n",
    "    - Loss ≈ 0.96\n",
    "- “전체 맞춘 비율(Accuracy)” ↓ / “모든 클래스를 공평하게 본 점수(Macro F1)” ↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d48afb5-7037-4a25-a746-b2b75079ea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2044\\1965059423.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# epoch 조정 (Early stopping)\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_wloss\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,          # 조금 낮추는 게 안정적\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,         # epoch\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "weighted_trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "\n",
    "# weight 조정\n",
    "weights = 1.0 / np.log(counts + 2)\n",
    "weights = weights / weights.mean()\n",
    "class_weights = torch.clamp(torch.tensor(weights), max=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d5b7297-4889-4bbc-bca3-f33d45573b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9175' max='27525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9175/27525 15:25 < 30:50, 9.92 it/s, Epoch 5/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>1.010115</td>\n",
       "      <td>0.667711</td>\n",
       "      <td>0.361112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.000300</td>\n",
       "      <td>0.987287</td>\n",
       "      <td>0.632153</td>\n",
       "      <td>0.364359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.027900</td>\n",
       "      <td>0.977763</td>\n",
       "      <td>0.666485</td>\n",
       "      <td>0.369032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.965500</td>\n",
       "      <td>0.967729</td>\n",
       "      <td>0.659946</td>\n",
       "      <td>0.367920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.964226</td>\n",
       "      <td>0.658174</td>\n",
       "      <td>0.367222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9175, training_loss=0.9216540128203764, metrics={'train_runtime': 925.2947, 'train_samples_per_second': 475.892, 'train_steps_per_second': 29.747, 'total_flos': 6105348843779856.0, 'train_loss': 0.9216540128203764, 'epoch': 5.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "851fc614-66b9-44a0-a740-fd062468c3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9777626991271973,\n",
       " 'eval_accuracy': 0.6664850136239782,\n",
       " 'eval_macro_f1': 0.3690322787233685,\n",
       " 'eval_runtime': 12.1247,\n",
       " 'eval_samples_per_second': 605.376,\n",
       " 'eval_steps_per_second': 37.857,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18859ab-34da-42c9-acc8-4df34d0e5789",
   "metadata": {},
   "source": [
    "### Stratified split 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c55fcfc-e7f5-4a19-b6f3-175dad801571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 36696\n",
      "After removing singleton labels: 36691\n",
      "Removed samples: 5\n",
      "Remaining unique labels: 60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# label이 정수(0~64)라고 가정\n",
    "label_counts = train[\"label\"].value_counts()\n",
    "singleton_labels = label_counts[label_counts < 2].index\n",
    "\n",
    "train_strat = train[~train[\"label\"].isin(singleton_labels)].copy()\n",
    "removed = len(train) - len(train_strat)\n",
    "\n",
    "print(\"Original:\", len(train))\n",
    "print(\"After removing singleton labels:\", len(train_strat))\n",
    "print(\"Removed samples:\", removed)\n",
    "print(\"Remaining unique labels:\", train_strat[\"label\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cc46940-0dba-4f69-aac5-fbf52845a925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 29352 Val: 7339\n",
      "Train unique labels: 60\n",
      "Val unique labels: 55\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train_strat,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=train_strat[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Val:\", len(val_df))\n",
    "print(\"Train unique labels:\", train_df[\"label\"].nunique())\n",
    "print(\"Val unique labels:\", val_df[\"label\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9a737e8-c69e-4f61-a2dc-effec6028a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights mean/max: 0.7720506191253662 10.0\n",
      "min count: 0 max count: 11841\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_labels = train[\"label\"].nunique()  # 65\n",
    "y_train_labels = train_df[\"label\"].values\n",
    "\n",
    "counts = np.bincount(y_train_labels, minlength=num_labels)\n",
    "weights = 1.0 / np.sqrt(counts + 1e-6)\n",
    "weights = weights / weights.mean()\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "class_weights = torch.clamp(class_weights, max=10.0)\n",
    "\n",
    "print(\"weights mean/max:\", class_weights.mean().item(), class_weights.max().item())\n",
    "print(\"min count:\", counts.min(), \"max count:\", counts.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3382ed9a-61e7-4e77-92a7-b3e5f56014bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa1c97b35df43e5bd983ff14439ffdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288961f158b94e8bbd5503b7279f0a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(['text',\n",
       "  'labels',\n",
       "  '__index_level_0__',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask'],\n",
       " ['text',\n",
       "  'labels',\n",
       "  '__index_level_0__',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]]).map(tokenize, batched=True)\n",
    "val_ds   = Dataset.from_pandas(val_df[[\"text\", \"label\"]]).map(tokenize, batched=True)\n",
    "\n",
    "# labels 컬럼명 맞추기\n",
    "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "val_ds   = val_ds.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_ds.column_names, val_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4329ce1c-3e2e-4ee5-a74b-c1449eb44c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2044\\1965059423.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "weighted_trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "833fe29c-19dc-4435-a54d-05ea4cf0b329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5505' max='27525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5505/27525 09:16 < 37:07, 9.88 it/s, Epoch 3/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.953600</td>\n",
       "      <td>0.866730</td>\n",
       "      <td>0.657719</td>\n",
       "      <td>0.388152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.837800</td>\n",
       "      <td>0.887156</td>\n",
       "      <td>0.663442</td>\n",
       "      <td>0.355953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.859849</td>\n",
       "      <td>0.646137</td>\n",
       "      <td>0.385571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='918' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_trainer.train()\n",
    "metrics = weighted_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5f102a8-5c1a-45c7-affb-cec7195f659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8667296767234802, 'eval_accuracy': 0.6577190352909116, 'eval_macro_f1': 0.3881515446159474, 'eval_runtime': 12.8891, 'eval_samples_per_second': 569.394, 'eval_steps_per_second': 35.611, 'epoch': 3.0}\n",
      "Best metric: 0.3881515446159474\n",
      "Best ckpt: ./results_wloss\\checkpoint-1835\n"
     ]
    }
   ],
   "source": [
    "metrics = weighted_trainer.evaluate()\n",
    "\n",
    "print(metrics)\n",
    "print(\"Best metric:\", weighted_trainer.state.best_metric)\n",
    "print(\"Best ckpt:\", weighted_trainer.state.best_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42bc1c85-5c93-4d17-83da-c0e28a033fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q] What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.] [E] i counted it and got 3 and a ha\n",
      "[Q] What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.] [A] \\( \\frac{1}{3} \\) [E] i counted\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 템플릿 2종 비교 (Q+E vs Q+A+E)\n",
    "\n",
    "def make_text_QE(df: pd.DataFrame) -> pd.Series:\n",
    "    return (\n",
    "        \"[Q] \" + df[\"QuestionText\"].fillna(\"\").astype(str) +\n",
    "        \" [E] \" + df[\"StudentExplanation\"].fillna(\"\").astype(str)\n",
    "    )\n",
    "\n",
    "def make_text_QAE(df: pd.DataFrame) -> pd.Series:\n",
    "    return (\n",
    "        \"[Q] \" + df[\"QuestionText\"].fillna(\"\").astype(str) +\n",
    "        \" [A] \" + df[\"MC_Answer\"].fillna(\"\").astype(str) +\n",
    "        \" [E] \" + df[\"StudentExplanation\"].fillna(\"\").astype(str)\n",
    "    )\n",
    "\n",
    "# train_strat / train_df / val_df 기준으로 text를 덮어씌워서 비교\n",
    "train_df_QE = train_df.copy()\n",
    "val_df_QE   = val_df.copy()\n",
    "train_df_QE[\"text\"] = make_text_QE(train_df_QE)\n",
    "val_df_QE[\"text\"]   = make_text_QE(val_df_QE)\n",
    "\n",
    "train_df_QAE = train_df.copy()\n",
    "val_df_QAE   = val_df.copy()\n",
    "train_df_QAE[\"text\"] = make_text_QAE(train_df_QAE)\n",
    "val_df_QAE[\"text\"]   = make_text_QAE(val_df_QAE)\n",
    "\n",
    "print(train_df_QE[\"text\"].iloc[0][:200])\n",
    "print(train_df_QAE[\"text\"].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71360f17-00e2-469a-96a5-4f64c1665828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "def run_experiment(train_df_in, val_df_in, class_weights, tag=\"exp\"):\n",
    "    # dataset\n",
    "    tr_ds = Dataset.from_pandas(train_df_in[[\"text\", \"label\"]]).map(tokenize, batched=True)\n",
    "    va_ds = Dataset.from_pandas(val_df_in[[\"text\", \"label\"]]).map(tokenize, batched=True)\n",
    "    tr_ds = tr_ds.rename_column(\"label\", \"labels\")\n",
    "    va_ds = va_ds.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    # 모델을 새로 로드(템플릿 비교는 같은 초기화에서 시작해야 공정)\n",
    "    model_local = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    trainer_local = WeightedTrainer(\n",
    "        model=model_local,\n",
    "        args=training_args,\n",
    "        train_dataset=tr_ds,\n",
    "        eval_dataset=va_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer_local.train()\n",
    "    metrics = trainer_local.evaluate()\n",
    "    best = trainer_local.state.best_metric\n",
    "\n",
    "    return metrics, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "251a6c24-7e72-4ebe-94ee-7e9bbda2e50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6267f9ff26434dc9b83c60c55bde14f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b435a2cf3324e9580b992db7bd56fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2044\\1965059423.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18350' max='27525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18350/27525 31:54 < 15:57, 9.58 it/s, Epoch 10/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.735000</td>\n",
       "      <td>1.437939</td>\n",
       "      <td>0.591225</td>\n",
       "      <td>0.220329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.106200</td>\n",
       "      <td>1.091641</td>\n",
       "      <td>0.671345</td>\n",
       "      <td>0.339543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.093000</td>\n",
       "      <td>1.002146</td>\n",
       "      <td>0.701185</td>\n",
       "      <td>0.383422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.945604</td>\n",
       "      <td>0.740428</td>\n",
       "      <td>0.432667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.871043</td>\n",
       "      <td>0.742608</td>\n",
       "      <td>0.447348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.922240</td>\n",
       "      <td>0.756779</td>\n",
       "      <td>0.469795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.975721</td>\n",
       "      <td>0.764954</td>\n",
       "      <td>0.499072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.925859</td>\n",
       "      <td>0.778853</td>\n",
       "      <td>0.515826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>0.929248</td>\n",
       "      <td>0.778853</td>\n",
       "      <td>0.510188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.950219</td>\n",
       "      <td>0.780352</td>\n",
       "      <td>0.511853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92448b57d8e4515a240c7e859f86e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0809243ff97648a5a46734aded3c6ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2044\\1965059423.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14680' max='27525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14680/27525 25:58 < 22:43, 9.42 it/s, Epoch 8/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.450100</td>\n",
       "      <td>1.203669</td>\n",
       "      <td>0.700095</td>\n",
       "      <td>0.281427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.779100</td>\n",
       "      <td>0.798312</td>\n",
       "      <td>0.785121</td>\n",
       "      <td>0.420543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.694191</td>\n",
       "      <td>0.782395</td>\n",
       "      <td>0.465213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.641936</td>\n",
       "      <td>0.817686</td>\n",
       "      <td>0.504992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>0.642246</td>\n",
       "      <td>0.833220</td>\n",
       "      <td>0.516771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.628093</td>\n",
       "      <td>0.846437</td>\n",
       "      <td>0.542483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.581700</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.852023</td>\n",
       "      <td>0.531896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.690792</td>\n",
       "      <td>0.862243</td>\n",
       "      <td>0.541876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QE ===\n",
      "{'eval_loss': 0.9258590340614319, 'eval_accuracy': 0.7788527047281646, 'eval_macro_f1': 0.5158261666078982, 'eval_runtime': 12.3807, 'eval_samples_per_second': 592.776, 'eval_steps_per_second': 37.074, 'epoch': 10.0}\n",
      "best macro_f1: 0.5158261666078982\n",
      "\n",
      "=== QAE ===\n",
      "{'eval_loss': 0.6280930042266846, 'eval_accuracy': 0.8464368442567107, 'eval_macro_f1': 0.5424830762375314, 'eval_runtime': 10.6871, 'eval_samples_per_second': 686.718, 'eval_steps_per_second': 42.949, 'epoch': 8.0}\n",
      "best macro_f1: 0.5424830762375314\n"
     ]
    }
   ],
   "source": [
    "metrics_QE, best_QE = run_experiment(train_df_QE, val_df_QE, class_weights, tag=\"QE\")\n",
    "metrics_QAE, best_QAE = run_experiment(train_df_QAE, val_df_QAE, class_weights, tag=\"QAE\")\n",
    "\n",
    "print(\"=== QE ===\")\n",
    "print(metrics_QE)\n",
    "print(\"best macro_f1:\", best_QE)\n",
    "\n",
    "print(\"\\n=== QAE ===\")\n",
    "print(metrics_QAE)\n",
    "print(\"best macro_f1:\", best_QAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f138d6d-fd3d-44e0-8718-ab9d66875372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df is val_df: False\n",
      "train_df_QE is val_df_QE: False\n",
      "train_df_QAE is val_df_QAE: False\n"
     ]
    }
   ],
   "source": [
    "print(\"train_df is val_df:\", train_df is val_df)\n",
    "print(\"train_df_QE is val_df_QE:\", train_df_QE is val_df_QE)\n",
    "print(\"train_df_QAE is val_df_QAE:\", train_df_QAE is val_df_QAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "222255ad-0ee7-4063-8bd4-b493853bf3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QE overlap count: 15\n",
      "QAE overlap count: 15\n"
     ]
    }
   ],
   "source": [
    "overlap_QE = set(train_df_QE[\"QuestionId\"]).intersection(\n",
    "    set(val_df_QE[\"QuestionId\"])\n",
    ")\n",
    "overlap_QAE = set(train_df_QAE[\"QuestionId\"]).intersection(\n",
    "    set(val_df_QAE[\"QuestionId\"])\n",
    ")\n",
    "\n",
    "print(\"QE overlap count:\", len(overlap_QE))\n",
    "print(\"QAE overlap count:\", len(overlap_QAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d43c674c-bf8a-4516-aec0-9c7a3ff638c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QE text overlap: 209\n",
      "QAE text overlap: 200\n"
     ]
    }
   ],
   "source": [
    "text_overlap_QE = set(train_df_QE[\"text\"]).intersection(\n",
    "    set(val_df_QE[\"text\"])\n",
    ")\n",
    "text_overlap_QAE = set(train_df_QAE[\"text\"]).intersection(\n",
    "    set(val_df_QAE[\"text\"])\n",
    ")\n",
    "\n",
    "print(\"QE text overlap:\", len(text_overlap_QE))\n",
    "print(\"QAE text overlap:\", len(text_overlap_QAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83189dac-74f2-4b76-9146-f7cce2457caf",
   "metadata": {},
   "source": [
    "- Data leakage 확인,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46001f97-f458-4cdf-a0bd-71fdda81bece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed: 5\n",
      "Train: 27014 Val: 9677\n",
      "QuestionId overlap: 0\n",
      "Text overlap: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# 1) singleton label 제거\n",
    "label_counts = train[\"label\"].value_counts()\n",
    "singleton_labels = label_counts[label_counts < 2].index\n",
    "train2 = train[~train[\"label\"].isin(singleton_labels)].copy()\n",
    "\n",
    "print(\"removed:\", len(train) - len(train2))\n",
    "\n",
    "# 2) group split\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "idx = np.arange(len(train2))\n",
    "train_idx, val_idx = next(gss.split(idx, groups=train2[\"QuestionId\"].values))\n",
    "\n",
    "train_df = train2.iloc[train_idx].copy()\n",
    "val_df   = train2.iloc[val_idx].copy()\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Val:\", len(val_df))\n",
    "print(\"QuestionId overlap:\", len(set(train_df[\"QuestionId\"]).intersection(set(val_df[\"QuestionId\"]))))\n",
    "print(\"Text overlap:\", len(set(train_df[\"text\"]).intersection(set(val_df[\"text\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6249e914-b3fa-45b8-87b1-179c7db42f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_text_QE(df: pd.DataFrame) -> pd.Series:\n",
    "    return (\n",
    "        \"[Q] \" + df[\"QuestionText\"].fillna(\"\").astype(str) +\n",
    "        \" [E] \" + df[\"StudentExplanation\"].fillna(\"\").astype(str)\n",
    "    )\n",
    "\n",
    "def make_text_QAE(df: pd.DataFrame) -> pd.Series:\n",
    "    return (\n",
    "        \"[Q] \" + df[\"QuestionText\"].fillna(\"\").astype(str) +\n",
    "        \" [A] \" + df[\"MC_Answer\"].fillna(\"\").astype(str) +\n",
    "        \" [E] \" + df[\"StudentExplanation\"].fillna(\"\").astype(str)\n",
    "    )\n",
    "\n",
    "# 일단 QAE로 진행 (원하면 QE로 바꿔도 됨)\n",
    "train = train.copy()\n",
    "train[\"text\"] = make_text_QAE(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d848e1e-e6b9-43e5-8722-c01176a106c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 36696\n",
      "After removing singleton labels: 36691\n",
      "Removed samples: 5\n",
      "Unique labels: 60\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_counts = train[\"label\"].value_counts()\n",
    "singleton_labels = label_counts[label_counts < 2].index\n",
    "\n",
    "train2 = train[~train[\"label\"].isin(singleton_labels)].copy()\n",
    "removed = len(train) - len(train2)\n",
    "\n",
    "print(\"Original:\", len(train))\n",
    "print(\"After removing singleton labels:\", len(train2))\n",
    "print(\"Removed samples:\", removed)\n",
    "print(\"Unique labels:\", train2[\"label\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e40da97-011f-4e21-a3ee-51b22e6c19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 27014 Val: 9677\n",
      "QuestionId overlap: 0\n",
      "Text overlap: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "idx = np.arange(len(train2))\n",
    "\n",
    "train_idx, val_idx = next(gss.split(idx, groups=train2[\"QuestionId\"].values))\n",
    "\n",
    "train_df = train2.iloc[train_idx].copy()\n",
    "val_df   = train2.iloc[val_idx].copy()\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Val:\", len(val_df))\n",
    "print(\"QuestionId overlap:\",\n",
    "      len(set(train_df[\"QuestionId\"]).intersection(set(val_df[\"QuestionId\"]))))\n",
    "print(\"Text overlap:\",\n",
    "      len(set(train_df[\"text\"]).intersection(set(val_df[\"text\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66290cc1-7508-4ad2-b7d3-21591f310227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min/max counts: 0 12119\n",
      "weights mean/max: 1.0 4.330390930175781\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_labels = train[\"label\"].nunique()\n",
    "counts = np.bincount(train_df[\"label\"].values, minlength=num_labels)\n",
    "\n",
    "weights = 1.0 / np.sqrt(counts + 1e-6)\n",
    "weights = weights / weights.mean()\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "class_weights = torch.clamp(class_weights, max=10.0)\n",
    "\n",
    "print(\"min/max counts:\", counts.min(), counts.max())\n",
    "print(\"weights mean/max:\", class_weights.mean().item(), class_weights.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c9b7d1e-abf3-4f82-9e26-ee3b7bbf5c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\hj\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19c7ff1e-63ab-42b6-bd7f-d0262bf70371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222086d9c20d4e26b4914eed9cb61474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a4edd6ac5a4a13abe214dea9544349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9677 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]]).map(tokenize, batched=True)\n",
    "val_ds   = Dataset.from_pandas(val_df[[\"text\", \"label\"]]).map(tokenize, batched=True)\n",
    "\n",
    "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "val_ds   = val_ds.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da01a993-1b78-4d09-b3f9-b4b76e46a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/group_split_wloss_QAE\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,              # 상한\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00ab7fb9-7160-4982-8470-c2c79f934ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2044\\1965059423.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7967648f-e654-4e78-8ca7-41dc05feb27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5067' max='25335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5067/25335 08:00 < 32:02, 10.54 it/s, Epoch 3/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.260800</td>\n",
       "      <td>5.495734</td>\n",
       "      <td>0.180738</td>\n",
       "      <td>0.033559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.012300</td>\n",
       "      <td>6.675133</td>\n",
       "      <td>0.155317</td>\n",
       "      <td>0.019553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.753700</td>\n",
       "      <td>7.871319</td>\n",
       "      <td>0.155213</td>\n",
       "      <td>0.019614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='605' max='605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [605/605 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.495733737945557, 'eval_accuracy': 0.1807378319727188, 'eval_macro_f1': 0.033559497140268235, 'eval_runtime': 13.0118, 'eval_samples_per_second': 743.709, 'eval_steps_per_second': 46.496, 'epoch': 3.0}\n",
      "Best metric: 0.033559497140268235\n",
      "Best ckpt: ./results/group_split_wloss_QAE\\checkpoint-1689\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(metrics)\n",
    "print(\"Best metric:\", trainer.state.best_metric)\n",
    "print(\"Best ckpt:\", trainer.state.best_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d804df1e-1f36-4803-8151-2b4e4673611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels: 65\n",
      "train label min/max: 0 64\n",
      "val   label min/max: 0 64\n",
      "unique labels train: 50\n",
      "unique labels val  : 16\n"
     ]
    }
   ],
   "source": [
    "print(\"num_labels:\", num_labels)\n",
    "print(\"train label min/max:\", train_df[\"label\"].min(), train_df[\"label\"].max())\n",
    "print(\"val   label min/max:\", val_df[\"label\"].min(), val_df[\"label\"].max())\n",
    "print(\"unique labels train:\", train_df[\"label\"].nunique())\n",
    "print(\"unique labels val  :\", val_df[\"label\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219b9af-6722-447d-8351-7962e8b74b3f",
   "metadata": {},
   "source": [
    "### 7. Validation 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ff039a5-57ec-49a5-8f40-a497377b77f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8327854871749878,\n",
       " 'eval_accuracy': 0.6673024523160763,\n",
       " 'eval_macro_f1': 0.373422873473576,\n",
       " 'eval_runtime': 8.4941,\n",
       " 'eval_samples_per_second': 864.128,\n",
       " 'eval_steps_per_second': 54.037,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98204e-9ca4-4e4e-be00-06f993836b87",
   "metadata": {},
   "source": [
    "### 8. Test Prediction & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d165279-41ab-4782-aa12-37b3caf508bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5597f536ca4c4f1089eee025d7b0b9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ds = Dataset.from_pandas(test[[\"text\"]]).map(tokenize, batched=True)\n",
    "\n",
    "test_preds = trainer.predict(test_ds)\n",
    "test_labels = np.argmax(test_preds.predictions, axis=1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"QuestionId\": test[\"QuestionId\"],\n",
    "    \"Category:Misconception\": le.inverse_transform(test_labels)\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_transformer.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
